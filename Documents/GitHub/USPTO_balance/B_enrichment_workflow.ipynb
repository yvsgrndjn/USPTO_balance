{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# for flattening tuples and lists\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "#import own modules\n",
    "import balancing_workflow as bw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(implement elsewhere, zenodo file maybe?) load GDB13S \n",
    "with open('/home/yves/Documents/GitHub/ReactionClasses/16_02_23/GDB/13S.smi', 'r') as f:\n",
    "    GDB13S = []\n",
    "    for line in f:\n",
    "        GDB13S.append(line.split('\\n')[0])\n",
    "\n",
    "#load formatted template dataframe\n",
    "#temp_r0 = pd.read_pickle('/home/yves/Documents/GitHub/USPTO_balance/data/amol_USPTO_templates_set_r0_v2_formatted.pkl')\n",
    "\n",
    "#load df_templates prepared in (*)\n",
    "df_templates = pd.read_pickle('/home/yves/Documents/GitHub/USPTO_balance/data/df_templates_to_enrich.pkl')\n",
    "\n",
    "#load GDB13S_mol (3%) prepared in (*)\n",
    "#with open('./data/GDB13S_003perc_mol.pkl', 'rb') as f:\n",
    "#    GDB13S_mol = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (*) 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*)\n",
    "#retrieve list of templates to work on\n",
    "templates_to_enrich, templates_to_enrich_appearances = bw.select_templates_to_enrich(temp_r0['template_hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*)\n",
    "#create a dataframe to store the enrichment information\n",
    "\n",
    "df_templates = pd.DataFrame(columns=['template_hash', 'frequency'])\n",
    "df_templates['template_hash'] = templates_to_enrich\n",
    "df_templates['frequency'] = templates_to_enrich_appearances\n",
    "df_templates['retro_templates'] = [bw.find_reaction_template_of_hash(temp_r0, templates_to_enrich[i]) for i in range(len(templates_to_enrich))] #3 min\n",
    "df_templates['retro_reac'] = [Chem.MolToSmiles(bw.rxn_smarts_to_sanitized_reactant_smarts(df_templates.at[i, 'retro_templates'])) for i in range(len(df_templates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*)\n",
    "#save df_templates to pickle \n",
    "df_templates.to_pickle('/home/yves/Documents/GitHub/USPTO_balance/data/df_templates_to_enrich.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDB13S_mol version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a subset of GDB13S ~3M SMILES\n",
    "random.seed(42)\n",
    "GDB13S = random.sample(GDB13S,round(0.03*len(GDB13S)))\n",
    "\n",
    "#convert SMILES to RDKit mol format\n",
    "from multiprocessing import Pool\n",
    "\n",
    "dataset = GDB13S\n",
    "processes = os.cpu_count()-2\n",
    "\n",
    "def MolFromSmiles(smi):\n",
    "    return Chem.MolFromSmiles(smi)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   with Pool(processes) as p:\n",
    "      output = list(tqdm(p.imap(MolFromSmiles, dataset), total = len(dataset)))\n",
    "\n",
    "GDB13S_mol = output\n",
    "del output, dataset\n",
    "\n",
    "with open('./data/GDB13S_003perc_mol.pkl', 'wb') as f:\n",
    "    pickle.dump(GDB13S_mol, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Framework dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prepare data to run all templates on a single portion of GDB13S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 993942/993942 [00:43<00:00, 22894.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#after loading GDB13S (full) and df_templates (full)\n",
    "\n",
    "#1. select a short enough part of GDB13S and save both the SMILES and the mol format\n",
    "\n",
    "fraction = 0.01 #input if function\n",
    "GDB_version = 1 #input if function\n",
    "percentage = fraction*100 \n",
    "\n",
    "random.seed(42)\n",
    "GDB13S = random.sample(GDB13S,round(fraction*len(GDB13S)))\n",
    "\n",
    "with open(f'./data/GDB13S_{GDB_version}.txt', 'w') as f:\n",
    "    for item in GDB13S:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. split df_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 10 different df_templates with 10 percent of the retro_reac each\n",
    "frac_retro_reac_per_split = 0.01 #input if function\n",
    "\n",
    "n_parts = int(1/frac_retro_reac_per_split)\n",
    "datasplits = list(split(list(df_templates['retro_reac'].unique()), n_parts))\n",
    "\n",
    "for i in range(n_parts):\n",
    "    df_templates_split = df_templates[df_templates['retro_reac'].isin(datasplits[i])]\n",
    "    df_templates_split.to_pickle(f'/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{i+1}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 100 config files for part 1 with same GDB version\n",
    "for i in range(n_parts):\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/config_files/config_part1_{i+1}.yaml', 'w') as f:\n",
    "        f.write(f'GDB13S_path: \"/home/yves/Documents/GitHub/USPTO_balance/data/GDB13S_{GDB_version}.txt\"\\n')\n",
    "        f.write(f'df_templates_path_to_pkl: \"/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{i+1}.pkl\"\\n')\n",
    "        f.write(f'GDB_version: \"{GDB_version}\"\\n')\n",
    "        f.write(f'template_version: \"{i+1}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN PART 1-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 100 config files for part 2 with same GDB version\n",
    "for i in range(n_parts):\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/config_files/config_part2_{i+1}.yaml', 'w') as f:\n",
    "        f.write(f'df_templates_path_to_pkl: \"/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{i+1}.pkl\"\\n')\n",
    "        f.write(f'GDB_version: \"{GDB_version}\"\\n')\n",
    "        f.write(f'template_version: \"{i+1}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN PART 2----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 100 config files for part 3 with same GDB version\n",
    "for i in range(n_parts):\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/config_files/config_part3_{i+1}.yaml', 'w') as f:\n",
    "        f.write(f'df_templates_path_to_pkl: \"/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{i+1}.pkl\"\\n')\n",
    "        f.write(f'GDB_version: \"{GDB_version}\"\\n')\n",
    "        f.write(f'template_version: \"{i+1}\"\\n')\n",
    "        f.write(f'Model_path_T2: \"/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T2_Reagent_Pred_225000.pt\"\\n')\n",
    "        f.write(f'Model_path_T3: \"/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T3_Forward_255000.pt\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN PART 3----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Development of reactions with confidence score > 0.9 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a list of reactions and makes them pass through T2\n",
    "# function that takes a list of reactions and makes them pass through T3\n",
    "# function that checks confidence score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that takes a list of reactions and makes them pass through T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# function that takes a list of reactions and makes them pass through T2 (1)\n",
    "\n",
    "#imports\n",
    "from ttlretro.single_step_retro import SingleStepRetrosynthesis\n",
    "singlestepretrosynthesis = SingleStepRetrosynthesis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a list of reactions and makes them pass through T2 (2)\n",
    "\n",
    "#let's load a random list (will be an input in the real function)\n",
    "GDB_version = '1'\n",
    "template_version = '1'\n",
    "retro_reac = '[C:1][C:2]'\n",
    "retro_template = '([CH2;D2;+0:1]-[CH2;D2;+0:2])>>([C;H0;D2;+0:1]#[C;H0;D2;+0:2])'\n",
    "\n",
    "with open(f'/home/yves/Documents/GitHub/USPTO_balance/created_rxns_{GDB_version}_{template_version}/rxns_{retro_reac}_{retro_template}.txt', 'r') as f:\n",
    "    rxns_list = []\n",
    "    for line in f:\n",
    "        rxns_list.append(line.split('\\n')[0])\n",
    "        \n",
    "rxns_list = rxns_list[:100] #ONLY FOR QUICK TESTING\n",
    "\n",
    "#associated function \n",
    "def load_rxns(GDB_version, template_version, retro_reac, retro_template):\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/created_rxns_{GDB_version}_{template_version}/rxns_{retro_reac}_{retro_template}.txt', 'r') as f:\n",
    "    rxns_list = []\n",
    "    for line in f:\n",
    "        rxns_list.append(line.split('\\n')[0])\n",
    "    return rxns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a list of reactions and makes them pass through T2 (3)\n",
    "\n",
    "#tokenize reactions\n",
    "tok_rxns_list = [singlestepretrosynthesis.smi_tokenizer(i) for i in rxns_list]\n",
    "\n",
    "#associated function\n",
    "def tokenize_rxn_list(rxns_list):\n",
    "    tok_rxns_list = [singlestepretrosynthesis.smi_tokenizer(i) for i in rxns_list]\n",
    "    return tok_rxns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a list of reactions and makes them pass through T2 (4)\n",
    "SMILES_list = tok_rxns_list \n",
    "Model_path  = '/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T2_Reagent_Pred_225000.pt'\n",
    "beam_size   = 1 \n",
    "batch_size  = 64\n",
    "untokenize_output = True\n",
    "[preds_T2, probs_T2] = singlestepretrosynthesis.Execute_Prediction(SMILES_list, Model_path, beam_size, batch_size, untokenize_output) #weird but results are under preds_T2[0], not preds_T2\n",
    "\n",
    "#associated function\n",
    "def run_T2_predictions(SMILES_list, Model_path, beam_size, batch_size, untokenize_output):\n",
    "    [preds_T2, probs_T2] = singlestepretrosynthesis.Execute_Prediction(SMILES_list, Model_path, beam_size, batch_size, untokenize_output)\n",
    "    return preds_T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that takes the T2 outputs and prepares them to pass through T3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes the T2 outputs and prepares them to pass through T3 (1)\n",
    "\n",
    "rxns_T2_list = [rxns_list[i].split('>>')[0] + '>' + preds_T2[0][i] + '>' + rxns_list[i].split('>>')[1] for i in range(len(preds_T2[0]))]\n",
    "rxns_T2_to_T3 = [rxns_list[i].split('>>')[0] + '>' + preds_T2[0][i] for i in range(len(preds_T2[0]))]\n",
    "rxns_T2_to_T3_tok = [singlestepretrosynthesis.smi_tokenizer(i) for i in rxns_T2_to_T3]\n",
    "\n",
    "#associated function\n",
    "def prepare_rxns_T2_for_T3(rxns_list, preds_T2):\n",
    "    rxns_T2_list = [rxns_list[i].split('>>')[0] + '>' + preds_T2[0][i] + '>' + rxns_list[i].split('>>')[1] for i in range(len(preds_T2[0]))]\n",
    "    rxns_T2_to_T3 = [rxns_list[i].split('>>')[0] + '>' + preds_T2[0][i] for i in range(len(preds_T2[0]))]\n",
    "    rxns_T2_to_T3_tok = [singlestepretrosynthesis.smi_tokenizer(i) for i in rxns_T2_to_T3]\n",
    "    return rxns_T2_list, rxns_T2_to_T3, rxns_T2_to_T3_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " function that takes a list of reactions and makes them pass through T3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "SMILES_list = rxns_T2_to_T3_tok \n",
    "Model_path  = '/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T3_Forward_255000.pt'\n",
    "beam_size   = 3 \n",
    "batch_size  = 64\n",
    "untokenize_output = True\n",
    "[preds_T3, probs_T3] = singlestepretrosynthesis.Execute_Prediction(SMILES_list, Model_path, beam_size, batch_size, untokenize_output)\n",
    "\n",
    "#associated function\n",
    "def run_T3_predictions(SMILES_list, Model_path, beam_size, batch_size, untokenize_output):\n",
    "    [preds_T3, probs_T3] = singlestepretrosynthesis.Execute_Prediction(SMILES_list, Model_path, beam_size, batch_size, untokenize_output)\n",
    "    return preds_T3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function comparing T3 predictions and reference and giving back the indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare preds_T3[0] with predictions reference\n",
    "\n",
    "#canonicalize preds_T3\n",
    "preds_T3[0] = [singlestepretrosynthesis.canonicalize_smiles(i) for i in preds_T3[0]]\n",
    "#preds_ref considered already canonicalized here\n",
    "preds_ref = [rxns_list[i].split('>>')[1] for i in range(len(rxns_list))]\n",
    "ind = [i for i in range(len(preds_T3[0])) if preds_T3[0][i] == preds_ref[i]]\n",
    "ind\n",
    "\n",
    "\n",
    "#associated function\n",
    "def find_ind_match_T3_preds_ref(preds_T3, rxns_list):\n",
    "    #canonicalize preds_T3\n",
    "    preds_T3 = [singlestepretrosynthesis.canonicalize_smiles(i) for i in preds_T3]\n",
    "    #preds_ref considered already canonicalized here\n",
    "    preds_ref = [rxns_list[i].split('>>')[1] for i in range(len(rxns_list))]\n",
    "    ind = [i for i in range(len(preds_T3)) if preds_T3[i] == preds_ref[i]]\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that gives back the full reactions with a confidence score > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_keep = [probs_T3[0][i] > 0.9 for i in range(len(probs_T3[0]))]\n",
    "rxns_conf = [rxns_T2_to_T3[i] for i in range(len(ind_keep)) if ind_keep[i] == True]\n",
    "\n",
    "#associated function\n",
    "def keeps_match_confident_rxns(rxns_T2_to_T3, probs_T3, conf_score: int = 90, ind:list):\n",
    "    ind_keep = [probs_T3[i] > conf_score for i in range(len(probs_T3))]\n",
    "    rxns_conf = [rxns_T2_to_T3[i] for i in range(len(ind_keep)) if ind_keep[i] == True and i in ind]\n",
    "    return rxns_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that saves the reactions with validated confidence scores replacing the old file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDB_version = '1'\n",
    "template_version = '1'\n",
    "retro_reac = '[C:1][C:2]'\n",
    "retro_template = '([CH2;D2;+0:1]-[CH2;D2;+0:2])>>([C;H0;D2;+0:1]#[C;H0;D2;+0:2])'\n",
    "\n",
    "with open(f'/home/yves/Documents/GitHub/USPTO_balance/created_rxns_{GDB_version}_{template_version}/rxns_{retro_reac}_{retro_template}.txt', 'w') as f:\n",
    "    for item in rxns_conf:\n",
    "        f.write(item + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "#associated function \n",
    "def save_conf_rxns(rxns_conf, GDB_version, template_version, retro_reac, retro_template):\n",
    "\n",
    "    folder_path = f'saved_rxns_{GDB_version}_{template_version}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/{folder_path}/rxns_{retro_reac}_{retro_template}.txt', 'w') as f:\n",
    "        for item in rxns_conf:\n",
    "            f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that reads a specific df_templates_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_version = '1'\n",
    "df_templates_split = pd.read_pickle(f'/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{template_version}.pkl')\n",
    "\n",
    "#associated function\n",
    "def load_template_version(template_version):\n",
    "    df_templates_split = pd.read_pickle(f'/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{template_version}.pkl')\n",
    "    return df_templates_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that returns the list of retro_reac and retro_template of a given df_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temp_and_retro_from_df(df_templates_split):\n",
    "    retro_templates =   [df_templates_split['retro_templates'].iloc[i] for i in range(len(df_templates_split))]\n",
    "    retro_reac =        [df_templates_split['retro_reac'].iloc[i] for i in range(len(df_templates_split))]\n",
    "    return retro_templates, retro_reac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template_hash</th>\n",
       "      <th>frequency</th>\n",
       "      <th>retro_templates</th>\n",
       "      <th>retro_reac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154a266ded329783e2edd97afda37dcc79a77eb501e212...</td>\n",
       "      <td>9594</td>\n",
       "      <td>([O;H0;D2;+0:2]-[c;H0;D3;+0:1])&gt;&gt;(Cl-[c;H0;D3;...</td>\n",
       "      <td>[cH3:1][OH:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d572f15ce843682d57bd39cd6c8575947b70a50f4c6527...</td>\n",
       "      <td>9461</td>\n",
       "      <td>([CH;D2;+0:1]=[O;H0;D1;+0:2])&gt;&gt;([CH2;D2;+0:1]-...</td>\n",
       "      <td>[C:1]=[O:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ab03ec2e113ad21265223676d461baf815168d6c7b2190...</td>\n",
       "      <td>8230</td>\n",
       "      <td>([C;H0;D3;+0:1]=[N;H0;D2;+0:2])&gt;&gt;(O=[C;H0;D3;+...</td>\n",
       "      <td>[CH2:1]=[NH:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>075d673cff5ec4531a74899eab1365e439a32f4c8b9037...</td>\n",
       "      <td>7853</td>\n",
       "      <td>([CH2;D2;+0:1]-[CH2;D2;+0:2])&gt;&gt;([CH;D2;+0:1]=[...</td>\n",
       "      <td>[C:1][C:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3c23d7484bb108274ce3044df7aec2c2dd6107ec5ca792...</td>\n",
       "      <td>7833</td>\n",
       "      <td>([CH2;D2;+0:1]-[n;H0;D3;+0:2])&gt;&gt;(Br-[CH2;D2;+0...</td>\n",
       "      <td>[C:1][nH2:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>8ca49036809ede2d44b731e140a50503fb00bf912ba207...</td>\n",
       "      <td>10</td>\n",
       "      <td>([C;H0;D3;+0:2]=[CH2;D1;+0:1])&gt;&gt;(Br-P(-[CH3;D1...</td>\n",
       "      <td>[C:1]=[CH2:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>b1bb6c849f1a4b49a0b872cfe028397cf16e47bb82dd65...</td>\n",
       "      <td>10</td>\n",
       "      <td>([C;H0;D3;+0:2]=[CH2;D1;+0:1])&gt;&gt;(C-N(-C)-[CH2;...</td>\n",
       "      <td>[C:1]=[CH2:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>4fd1f87303fb64d23793a5141afe558cc8e8706f8862d7...</td>\n",
       "      <td>10</td>\n",
       "      <td>([C;H0;D3;+0:1]-[NH;D2;+0:2])&gt;&gt;(C-O-C(=O)-C-N-...</td>\n",
       "      <td>[CH3:1][N:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>540e2d1754cd1abfe987ca68d05a0d3e0ab395ec72ee9e...</td>\n",
       "      <td>10</td>\n",
       "      <td>([c;H0;D3;+0:1]-[c;H0;D3;+0:2])&gt;&gt;(O-[c;H0;D3;+...</td>\n",
       "      <td>[cH3:1]-[cH3:2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>595c50cf653434fe012465461e668a6067d7d1bc03bc5c...</td>\n",
       "      <td>10</td>\n",
       "      <td>([CH2;D2;+0:1]-[CH;D2;+0:2])&gt;&gt;(O-C/C=C\\[CH2;D2...</td>\n",
       "      <td>[C:1][C:2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          template_hash  ...       retro_reac\n",
       "0     154a266ded329783e2edd97afda37dcc79a77eb501e212...  ...    [cH3:1][OH:2]\n",
       "1     d572f15ce843682d57bd39cd6c8575947b70a50f4c6527...  ...      [C:1]=[O:2]\n",
       "2     ab03ec2e113ad21265223676d461baf815168d6c7b2190...  ...   [CH2:1]=[NH:2]\n",
       "3     075d673cff5ec4531a74899eab1365e439a32f4c8b9037...  ...       [C:1][C:2]\n",
       "4     3c23d7484bb108274ce3044df7aec2c2dd6107ec5ca792...  ...     [C:1][nH2:2]\n",
       "...                                                 ...  ...              ...\n",
       "5540  8ca49036809ede2d44b731e140a50503fb00bf912ba207...  ...    [C:1]=[CH2:2]\n",
       "5549  b1bb6c849f1a4b49a0b872cfe028397cf16e47bb82dd65...  ...    [C:1]=[CH2:2]\n",
       "5553  4fd1f87303fb64d23793a5141afe558cc8e8706f8862d7...  ...     [CH3:1][N:2]\n",
       "5554  540e2d1754cd1abfe987ca68d05a0d3e0ab395ec72ee9e...  ...  [cH3:1]-[cH3:2]\n",
       "5555  595c50cf653434fe012465461e668a6067d7d1bc03bc5c...  ...       [C:1][C:2]\n",
       "\n",
       "[756 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_templates_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ttlretro.single_step_retro import SingleStepRetrosynthesis\n",
    "singlestepretrosynthesis = SingleStepRetrosynthesis()\n",
    "\n",
    "\n",
    "def load_rxns(GDB_version, template_version, retro_reac, retro_template):\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/created_rxns_{GDB_version}_{template_version}/rxns_{retro_reac}_{retro_template}.txt', 'r') as f:\n",
    "        rxns_list = []\n",
    "        for line in f:\n",
    "            rxns_list.append(line.split('\\n')[0])\n",
    "    return rxns_list\n",
    "\n",
    "\n",
    "def tokenize_rxn_list(rxns_list):\n",
    "    tok_rxns_list = [singlestepretrosynthesis.smi_tokenizer(i) for i in rxns_list]\n",
    "    return tok_rxns_list\n",
    "\n",
    "\n",
    "def run_T2_predictions(SMILES_list, Model_path, beam_size: int = 1, batch_size: int = 64, untokenize_output:bool = True):\n",
    "    [preds_T2, probs_T2] = singlestepretrosynthesis.Execute_Prediction(SMILES_list, Model_path, beam_size, batch_size, untokenize_output)\n",
    "    return preds_T2[0]\n",
    "\n",
    "\n",
    "def prepare_rxns_T2_for_T3(rxns_list, preds_T2):\n",
    "    rxns_T2_list = [rxns_list[i].split('>>')[0] + '>' + preds_T2[0][i] + '>' + rxns_list[i].split('>>')[1] for i in range(len(preds_T2))]\n",
    "    rxns_T2_to_T3 = [rxns_list[i].split('>>')[0] + '>' + preds_T2[0][i] for i in range(len(preds_T2))]\n",
    "    rxns_T2_to_T3_tok = [singlestepretrosynthesis.smi_tokenizer(i) for i in rxns_T2_to_T3]\n",
    "    return rxns_T2_list, rxns_T2_to_T3, rxns_T2_to_T3_tok\n",
    "\n",
    "\n",
    "def run_T3_predictions(SMILES_list, Model_path, beam_size: int = 3, batch_size: int = 64, untokenize_output:bool = True):\n",
    "    [preds_T3, probs_T3] = singlestepretrosynthesis.Execute_Prediction(SMILES_list, Model_path, beam_size, batch_size, untokenize_output)\n",
    "    return preds_T3[0], probs_T3[0]\n",
    "\n",
    "\n",
    "def find_ind_match_T3_preds_ref(preds_T3, rxns_list):\n",
    "    #canonicalize preds_T3\n",
    "    preds_T3 = [singlestepretrosynthesis.canonicalize_smiles(i) for i in preds_T3]\n",
    "    preds_ref = [rxns_list[i].split('>>')[1] for i in range(len(rxns_list))]\n",
    "    ind = [i for i in range(len(preds_T3)) if preds_T3[i] == preds_ref[i]]\n",
    "    return ind\n",
    "\n",
    "\n",
    "def keeps_match_confident_rxns(rxns_T2_to_T3, probs_T3, conf_score: int = 90, match_ind: list):\n",
    "    ind_keep = [probs_T3[i] > conf_score for i in range(len(probs_T3))]\n",
    "    rxns_conf = [rxns_T2_to_T3[i] for i in range(len(ind_keep)) if ind_keep[i] == True and i in match_ind]\n",
    "    return rxns_conf\n",
    "\n",
    "\n",
    "def save_conf_rxns(rxns_conf, GDB_version, template_version, retro_reac, retro_template):\n",
    "\n",
    "    folder_path = f'saved_rxns_{GDB_version}_{template_version}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(f'/home/yves/Documents/GitHub/USPTO_balance/{folder_path}/rxns_{retro_reac}_{retro_template}.txt', 'w') as f:\n",
    "        for item in rxns_conf:\n",
    "            f.write(item + '\\n')\n",
    "\n",
    "\n",
    "def load_template_version(template_version):\n",
    "    df_templates_split = pd.read_pickle(f'/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_{template_version}.pkl')\n",
    "    return df_templates_split\n",
    "\n",
    "\n",
    "def delete_evaluated_rxns(GDB_version, template_version, retro_reac, retro_template):\n",
    "\n",
    "    name = f'rxns_{retro_reac}_{retro_template}'\n",
    "    folder_path = f'./created_rxns_{GDB_version}_{template_version}'\n",
    "    os.remove(f'{folder_path}/{name}.txt')\n",
    "\n",
    "\n",
    "def read_config(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def reactions_conf_validation(GDB_version, template_version, retro_reac, retro_template, Model_path_T2, Model_path_T3):\n",
    "\n",
    "    rxns_list = load_rxns(GDB_version, template_version, retro_reac, retro_template)\n",
    "    tok_rxns_list = tokenize_rxn_list(rxns_list)\n",
    "    preds_T2 = run_T2_predictions(tok_rxns_list, Model_path_T2, beam_size = 1, batch_size = 64, untokenize_output = True)\n",
    "    rxns_T2_list, rxns_T2_to_T3, rxns_T2_to_T3_tok = prepare_rxns_T2_for_T3(rxns_list, preds_T2)\n",
    "    preds_T3, probs_T3 = run_T3_predictions(rxns_T2_to_T3_tok, Model_path_T3, beam_size = 3, batch_size = 64, untokenize_output = True)\n",
    "    ind_match = find_ind_match_T3_preds_ref(preds_T3, rxns_list)\n",
    "    rxns_conf = keeps_match_confident_rxns(rxns_T2_to_T3, probs_T3, ind_match, conf_score = 0.9)\n",
    "    save_conf_rxns(rxns_conf, GDB_version, template_version, retro_reac, retro_template)\n",
    "    #delete_evaluated_rxns(GDB_version, template_version, retro_reac, retro_template)\n",
    "\n",
    "def main(GDB_version, template_version, Model_path_T2, Model_path_T3):\n",
    "\n",
    "    df_templates_split = load_template_version(template_version)\n",
    "\n",
    "    for retro_reac, retro_template in tqdm(zip(df_templates_split['retro_reac'], df_templates_split['retro_templates'])):\n",
    "        reactions_conf_validation(GDB_version, template_version, retro_reac, retro_template, Model_path_T2, Model_path_T3)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c', '--config', help='Path to the configuration file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.config:\n",
    "        print('Please provide a configuration file')\n",
    "        sys.exit()\n",
    "    elif not os.path.exists(args.config):\n",
    "        print('The configuration file does not exist')\n",
    "        sys.exit()\n",
    "    config = read_config(args.config)\n",
    "\n",
    "    main(\n",
    "        config['GDB_version'],\n",
    "        config['template_version'],\n",
    "        config['Model_path_T2'],\n",
    "        config['Model_path_T3']        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs in the config file\n",
    "template_version = '1'\n",
    "GDB_version = '1'\n",
    "Model_path_T2 = '/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T2_Reagent_Pred_225000.pt'\n",
    "Model_path_T3 = '/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T3_Forward_255000.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test part 3 of the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config file\n",
    "template_version = '1'\n",
    "GDB_version = '1'\n",
    "Model_path_T2 = '/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T2_Reagent_Pred_225000.pt'\n",
    "Model_path_T3 = '/home/yves/Documents/GitHub/TTL_versions/1.4/models/USPTO_STEREO_separated_T3_Forward_255000.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_templates_split = bw.load_template_version(template_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose retro_reac and retro_template (done in the for loop for all couples in df_templates_split)\n",
    "retro_reac = '[C:1][C:2]'\n",
    "retro_template = '([CH2;D2;+0:1]-[CH2;D2;+0:2])>>([CH;D2;+0:1]=[CH;D2;+0:2])'\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxns_list = bw.load_rxns(GDB_version, template_version, retro_reac, retro_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep a subset to go faster (only for testing)\n",
    "rxns_list = rxns_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tok_rxns_list = bw.tokenize_rxn_list(rxns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "preds_T2 = bw.run_T2_predictions(tok_rxns_list, Model_path_T2, beam_size = 1, batch_size = 64, untokenize_output = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rxns_list), len(preds_T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "rxns_T2_list, rxns_T2_to_T3, rxns_T2_to_T3_tok = bw.prepare_rxns_T2_for_T3(rxns_list, preds_T2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "preds_T3, probs_T3 = bw.run_T3_predictions(rxns_T2_to_T3_tok, Model_path_T3, beam_size = 3, batch_size = 64, untokenize_output = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ind_match = bw.find_ind_match_T3_preds_ref(preds_T3, rxns_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxns_conf = bw.keeps_match_confident_rxns(rxns_T2_to_T3, probs_T3, ind_match, conf_score = 0.9) #adapt arguments to added function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/yves/anaconda3/envs/MultiStepRetro/lib/python3.8/site-packages/rxnmapper/models/transformers/albert_heads_8_uspto_all_1310k were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bw.save_conf_rxns(rxns_conf, GDB_version, template_version, retro_reac, retro_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create function that deletes files that were looked through already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_evaluated_rxns(GDB_version, template_version, retro_reac, retro_template):\n",
    "\n",
    "    name = f'rxns_{retro_reac}_{retro_template}'\n",
    "    folder_path = f'./created_rxns_{GDB_version}_{template_version}'\n",
    "    os.remove(f'{folder_path}/{name}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_evaluated_rxns(GDB_version, template_version, retro_reac, retro_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Add canonicalization in format_reaction function (both sides or only right side?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 compare part 1 results with and after canonicalization of the subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Watch impact of canonicalization on part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "df_templates_path = \"/home/yves/Documents/GitHub/USPTO_balance/data/templates_split/df_templates_to_enrich_part_1.pkl\"\n",
    "GDB_version = \"1\"\n",
    "template_version = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_templates = pd.read_pickle(df_templates_path)\n",
    "\n",
    "#insert manually both retro_reac and retro_template as it is in a for loop usually\n",
    "retro_reac = '[C:1][C:2]'\n",
    "retro_template = '([CH2;D2;+0:1]-[CH2;D2;+0:2])>>([CH;D2;+0:1]=[CH;D2;+0:2])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDB13S_sub, GDB13S_sub_mol = bw.load_subsets(retro_reac, GDB_version, template_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorten a bit the datasets for testing\n",
    "GDB13S_sub = GDB13S_sub[:100]\n",
    "GDB13S_sub_mol = GDB13S_sub_mol[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDB13S_sub_app_temp = bw.apply_rxn_template_on_mols_list(GDB13S_sub_mol, retro_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_remove = [GDB13S_sub_app_temp[i] == () for i in range(len(GDB13S_sub_app_temp))]\n",
    "GDB13S_sub_app_temp_sort = [GDB13S_sub_app_temp[i] for i in range(len(GDB13S_sub_app_temp)) if not ind_remove[i]]\n",
    "GDB13S_sub_sort = [GDB13S_sub[i] for i in range(len(GDB13S_sub)) if not ind_remove[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "fictive_rxns_list = [bw.format_reaction(GDB13S_sub_app_temp_sort[k], GDB13S_sub_sort[k]) for k in range(len(GDB13S_sub_sort))]\n",
    "fictive_rxns_list = list(chain.from_iterable(fictive_rxns_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
